{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1O3nT7ieau-o"
   },
   "source": [
    "# Testing Captum  + Text Generation\n",
    "\n",
    "This notebook was created to test the BERTViz package for attention visualization.\n",
    "\n",
    "### Tested Model\n",
    "- GPT-2\n",
    "- GODEL\n",
    "- Mistral 7B Instruct\n",
    "- LlaMa 2 7B Chat (HF Version)\n",
    "\n",
    "#### Tested Attention Visualizations\n",
    "\n",
    "### Hardware Acceleration\n",
    "This was run on a hardware accelerated google colab notebook with 50GB of RAM.**Using less RAM will lead to issues.** Also loading all models in the same session will lead to crashes (i.e. Mistral Instruct takes up 30GB of Memory alone).\n",
    "\n",
    "\n",
    "Additionally a GPU can be used, however model interference is reasonably fast on pure gpu performance."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installation, Imports & Setup"
   ],
   "metadata": {
    "id": "hCxI40PF7ikK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokens for Downloads\n",
    "\n",
    "Without a Github token the different variant of shap cannot be loaded. Without a HGF Token llama cannot load from the huggingface hub.\n",
    "\n",
    "This is set up for colab, alternatively the commented string variant below can be used. For this replace the string with an actual token.\n",
    "\n",
    "*   Github [Token Info](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens)\n",
    "*   Huggingface [Token Info](https://huggingface.co/docs/hub/security-tokens)"
   ],
   "metadata": {
    "id": "GbORn8fuwT_G"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# grabbing tokens for repository and model access\n",
    "from google.colab import userdata\n",
    "\n",
    "gh_token = userdata.get(\"GITHUB_TOKEN\")\n",
    "hgf_token = userdata.get(\"HGF_TOKEN\")\n",
    "\n",
    "# gh_token=\"TOKEN\"\n",
    "# hgf_token=\"TOKEN\""
   ],
   "metadata": {
    "id": "GOyuLH8dwUWG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706363813215,
     "user_tz": -60,
     "elapsed": 2046,
     "user": {
      "displayName": "Lennard Zündorf",
      "userId": "01714560816823084743"
     }
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yY4-ISNMAs1t"
   },
   "outputs": [],
   "source": [
    "# basic installs and additional utilies (usually not needed in colab)\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install ipywidgets\n",
    "!pip install ipython\n",
    "\n",
    "# model package installs\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install huggingface_hub\n",
    "!pip install accelerate\n",
    "!pip install sklearn\n",
    "\n",
    "# bertbiz install\n",
    "!pip install bertviz"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# basic imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# model imports\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# interpretability import\n",
    "import bertviz\n",
    "import IPython"
   ],
   "metadata": {
    "id": "UX_0O6SKNsYJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706363886633,
     "user_tz": -60,
     "elapsed": 6811,
     "user": {
      "displayName": "Lennard Zündorf",
      "userId": "01714560816823084743"
     }
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup Models"
   ],
   "metadata": {
    "id": "UuaL0U5EOiRl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# setting device based on available hardware\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device set to {device}.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BtDRo6GswgjK",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706363886634,
     "user_tz": -60,
     "elapsed": 10,
     "user": {
      "displayName": "Lennard Zündorf",
      "userId": "01714560816823084743"
     }
    },
    "outputId": "eec6563f-6858-4061-9808-23be087a776b"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device set to cpu.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# setup gpt-2 and godel model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "\n",
    "# gpt and godel loading function so this can be run individually\n",
    "def load_gd_gpt():\n",
    "\n",
    "    # load tokenizer and model from huggingface\n",
    "    gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "    gpt_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # manage setup based on available device\n",
    "    gpt_model.to(device)\n",
    "\n",
    "    # update model config\n",
    "    gpt_model.config.is_decoder = True\n",
    "    gpt_model.config.max_new_tokens = 50\n",
    "    gpt_model.config.do_sample = True\n",
    "\n",
    "    # load tokenizer and model from huggingface\n",
    "    gd_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/GODEL-v1_1-large-seq2seq\")\n",
    "    gd_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        \"microsoft/GODEL-v1_1-large-seq2seq\"\n",
    "    )\n",
    "\n",
    "    # manage setup based on available device\n",
    "    gd_model.to(device)\n",
    "\n",
    "    # update GODEL model config\n",
    "    gd_model.config.max_new_tokens = 50\n",
    "    gd_model.config.do_sample = True\n",
    "\n",
    "    return gpt_model, gpt_tokenizer, gd_model, gd_tokenizer"
   ],
   "metadata": {
    "id": "_Vm0vDtFNxG_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706364379178,
     "user_tz": -60,
     "elapsed": 5,
     "user": {
      "displayName": "Lennard Zündorf",
      "userId": "01714560816823084743"
     }
    }
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# mistral loading function, so this doesn't run automatically on load\n",
    "def load_mistral():\n",
    "\n",
    "    # load tokenizer and model from huggingface\n",
    "    mistral_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    )\n",
    "    mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    )\n",
    "\n",
    "    # manage setup based on available device\n",
    "    mistral_model.to(device)\n",
    "\n",
    "    # update model config\n",
    "    mistral_model.config.is_decoder = True\n",
    "    mistral_model.config.max_length = 50\n",
    "    mistral_model.config.no_repeat_ngram_size = 2\n",
    "    mistral_model.config.do_sample = True\n",
    "\n",
    "    return mistral_model, mistral_tokenizer"
   ],
   "metadata": {
    "id": "FAW2kd9cNxkY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# llama loading function, so this doesn't run automatically on load\n",
    "def load_llama():\n",
    "\n",
    "    # load tokenizer and model from huggingface\n",
    "    llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"meta-llama/Llama-2-7b-chat-hf\", token=hgf_token\n",
    "    )\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-2-7b-chat-hf\", token=hgf_token\n",
    "    )\n",
    "\n",
    "    # manage setup based on available device\n",
    "    llama_model.to(device)\n",
    "\n",
    "    # update model config\n",
    "    llama_model.config.is_decoder = True\n",
    "    llama_model.config.max_length = 50\n",
    "    llama_model.config.no_repeat_ngram_size = 2\n",
    "    llama_model.config.do_sample = True\n",
    "\n",
    "    # update tokenizer config\n",
    "    llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "\n",
    "    return llama_model, llama_tokenizer"
   ],
   "metadata": {
    "id": "ptIXuMdJN0ml"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**(Loading all Models in Parallel will overload the 50GB RAM)**\n",
    "\n",
    "-> load either gpt + GODAL or Mistral or Llama2"
   ],
   "metadata": {
    "id": "exxmi7egOBYT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# loading gpt and godel model and tokenizer\n",
    "gpt_model, gpt_tokenizer, gd_model, gd_tokenizer = load_gd_gpt()"
   ],
   "metadata": {
    "id": "jdpIDKAWN2AN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706364395763,
     "user_tz": -60,
     "elapsed": 12496,
     "user": {
      "displayName": "Lennard Zündorf",
      "userId": "01714560816823084743"
     }
    }
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# loading mistral model and tokenizer\n",
    "mistral_model, mistral_tokenizer = load_mistral()"
   ],
   "metadata": {
    "id": "zoevIdalwr1a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# loading llama model and tokenizer\n",
    "llama_model, llama_tokenizer = load_llama()"
   ],
   "metadata": {
    "id": "AAhHfn3Hwu17"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing Code"
   ],
   "metadata": {
    "id": "8X1X5uZG8FhO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper Functions"
   ],
   "metadata": {
    "id": "NayCYdtY8N-j"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# function to format the model reponse nicely\n",
    "def format_output_text(output: list):\n",
    "    # remove special tokens from list\n",
    "    formatted_output = format_tokens(output)\n",
    "\n",
    "    # start string with first list item if it is not empty\n",
    "    if formatted_output[0] != \"\":\n",
    "        output_str = formatted_output[0]\n",
    "    else:\n",
    "        # alternatively start with second list item\n",
    "        output_str = formatted_output[1]\n",
    "\n",
    "    # add all other list items with a space in between\n",
    "    for txt in formatted_output[1:]:\n",
    "        # check if the token is a punctuation mark\n",
    "        if txt in [\".\", \",\", \"!\", \"?\"]:\n",
    "            # add punctuation mark without space\n",
    "            output_str += txt\n",
    "        # add token with space if not empty\n",
    "        elif txt != \"\":\n",
    "            output_str += \" \" + txt\n",
    "\n",
    "    # return the combined string with multiple spaces removed\n",
    "    return re.sub(\" +\", \" \", output_str)\n",
    "\n",
    "\n",
    "# format the tokens by removing special tokens and special characters\n",
    "def format_tokens(tokens: list):\n",
    "    # define special tokens to remove and initialize empty list\n",
    "    special_tokens = [\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[UNK]\", \"[MASK]\", \"▁\", \"Ġ\", \"</w>\"]\n",
    "    updated_tokens = []\n",
    "\n",
    "    # loop through tokens\n",
    "    for t in tokens:\n",
    "        # remove special token from start of token if found\n",
    "        if t.startswith(\"▁\"):\n",
    "            t = t.lstrip(\"▁\")\n",
    "\n",
    "        # loop through special tokens and remove them if found\n",
    "        for s in special_tokens:\n",
    "            t = t.replace(s, \"\")\n",
    "\n",
    "        # add token to list\n",
    "        updated_tokens.append(t)\n",
    "\n",
    "    # return the list of tokens\n",
    "    return updated_tokens\n",
    "\n",
    "\n",
    "def avg_attention(attention_values):\n",
    "    attention = attention_values.cross_attentions[0][0].detach().numpy()\n",
    "    return np.mean(attention, axis=0)"
   ],
   "metadata": {
    "id": "bH_IwxL88Rbz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706363887001,
     "user_tz": -60,
     "elapsed": 7,
     "user": {
      "displayName": "Lennard Zündorf",
      "userId": "01714560816823084743"
     }
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bh8YtbD_aqs1"
   },
   "source": [
    "### Testing BertViz with GPT-2\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# defining test_input\n",
    "test_input = \"Harry is a lawyer on the east coast, his hobbies include\""
   ],
   "metadata": {
    "id": "Zq7STXsdyHL5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVzRZV9qDXa1"
   },
   "outputs": [],
   "source": [
    "# creating a model and head view for GPT-2\n",
    "# CREDIT: adopted from the official BERTViz documentation\n",
    "## see: https://github.com/jessevig/bertviz\n",
    "\n",
    "# imports\n",
    "from transformers import utils\n",
    "from bertviz import model_view, head_view\n",
    "\n",
    "# generating a model output with attentions\n",
    "inputs = gpt_tokenizer(test_input, return_tensors=\"pt\")\n",
    "out = gpt_model(**inputs, output_attentions=True)\n",
    "\n",
    "# extracting attention from model output\n",
    "attention = out[\"attentions\"]  # Retrieve attention from model outputs\n",
    "tokens = gpt_tokenizer.convert_ids_to_tokens(\n",
    "    inputs[\"input_ids\"][0]\n",
    ")  # Convert input ids to token strings\n",
    "\n",
    "# creating model and head view using BERTViz\n",
    "mview = model_view(attention, tokens)\n",
    "hview = head_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SkA00qhQUZh"
   },
   "outputs": [],
   "source": [
    "# creating a neuron view for GPT-2\n",
    "# CREDIT: copied from the offical BERTViz documentation\n",
    "## see: https://github.com/jessevig/bertviz/blob/master/notebooks/neuron_view_gpt2.ipynb\n",
    "\n",
    "# imports\n",
    "from bertviz import neuron_view as nv\n",
    "from bertviz.transformers_neuron_view import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "# setting model type and version\n",
    "model_type = \"gpt2\"\n",
    "model_version = \"gpt2\"\n",
    "\n",
    "# creating model and tokenizer from special BERTViz gpt classes\n",
    "model = GPT2Model.from_pretrained(model_version)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_version)\n",
    "\n",
    "nv.show(model, model_type, tokenizer, test_input, display_mode=\"dark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generalized Testing Functions"
   ],
   "metadata": {
    "id": "J2Ih6vDGNkZr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# creating a model and head view for GODEL\n",
    "# CREDIT: adopted from the official BERTViz documentation\n",
    "## see: https://github.com/jessevig/bertviz\n",
    "\n",
    "# imports\n",
    "from transformers import utils\n",
    "from bertviz import model_view, head_view\n",
    "\n",
    "\n",
    "def hview(test_input, model, tokenizer):\n",
    "\n",
    "    # generating a model output with attentions\n",
    "    inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
    "    out = model(**inputs, output_attentions=True)\n",
    "\n",
    "    # extracting attention from model output\n",
    "    attention = out[\"attentions\"]  # Retrieve attention from model outputs\n",
    "    tokens = tokenizer.convert_ids_to_tokens(\n",
    "        inputs[\"input_ids\"][0]\n",
    "    )  # Convert input ids to token strings\n",
    "\n",
    "    # creating head view using BERTViz\n",
    "    view = head_view(attention, tokens, html_action=\"view\")\n",
    "\n",
    "\n",
    "def mview(test_input, model, tokenizer):\n",
    "\n",
    "    # generating a model output with attentions\n",
    "    inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
    "    out = model(**inputs, output_attentions=True)\n",
    "\n",
    "    # extracting attention from model output\n",
    "    attention = out[\"attentions\"]  # Retrieve attention from model outputs\n",
    "    tokens = tokenizer.convert_ids_to_tokens(\n",
    "        inputs[\"input_ids\"][0]\n",
    "    )  # Convert input ids to token strings\n",
    "\n",
    "    # creating model and head view using BERTViz\n",
    "    model_view(attention, tokens, html_action=\"view\")"
   ],
   "metadata": {
    "id": "bE-b_pmvyiCP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing BERTViz with GODEL"
   ],
   "metadata": {
    "id": "nk-oPAGSyikd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# formatting function to formatting input for the model\n",
    "# CREDIT: Adapted from official interference example on Huggingface\n",
    "## see https://huggingface.co/microsoft/GODEL-v1_1-large-seq2seq\n",
    "def gd_format_prompt(message: str, system_prompt: str, knowledge: str = \"\"):\n",
    "\n",
    "    # adds knowledge text if not empty\n",
    "    if knowledge != \"\":\n",
    "        knowledge = \"[KNOWLEDGE] \" + knowledge\n",
    "\n",
    "    # adds the message to the prompt\n",
    "    prompt = f\" {message}\"\n",
    "    # combines the entire prompt\n",
    "    full_prompt = f\"{system_prompt} [CONTEXT] {prompt} {knowledge}\"\n",
    "\n",
    "    # returns the formatted prompt\n",
    "    return full_prompt"
   ],
   "metadata": {
    "id": "BUhz-KY-yttH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706363887002,
     "user_tz": -60,
     "elapsed": 7,
     "user": {
      "displayName": "Lennard Zündorf",
      "userId": "01714560816823084743"
     }
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# special godel model view function because of seq2seq nature\n",
    "# CREDIT: Adapted from offical BERTViz model view example for encoder-decoder models\n",
    "## see https://github.com/jessevig/bertviz/blob/master/notebooks/model_view_encoder_decoder.ipynb\n",
    "\n",
    "# imports\n",
    "from bertviz import model_view\n",
    "\n",
    "# model view function for encoder decoder models\n",
    "\n",
    "\n",
    "def mview_enc_dec(test_input, model, tokenizer):\n",
    "    # generating encoder and encoder inputs through tokenization\n",
    "    encoder_input_ids = tokenizer(\n",
    "        test_input, return_tensors=\"pt\", add_special_tokens=True\n",
    "    ).input_ids\n",
    "    decoder_input_ids = model.generate(encoder_input_ids, output_attentions=True)\n",
    "\n",
    "    # generate model output\n",
    "    outputs = model(\n",
    "        input_ids=encoder_input_ids,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        output_attentions=True,\n",
    "    )\n",
    "\n",
    "    # get the text for the input and output vectors\n",
    "    encoder_text = format_tokens(tokenizer.convert_ids_to_tokens(encoder_input_ids[0]))\n",
    "    decoder_text = format_tokens(tokenizer.convert_ids_to_tokens(decoder_input_ids[0]))\n",
    "\n",
    "    model_view(\n",
    "        encoder_attention=outputs.encoder_attentions,\n",
    "        decoder_attention=outputs.decoder_attentions,\n",
    "        cross_attention=outputs.cross_attentions,\n",
    "        encoder_tokens=encoder_text,\n",
    "        decoder_tokens=decoder_text,\n",
    "        html_action=\"view\",\n",
    "    )"
   ],
   "metadata": {
    "id": "4s4VKB9pzL7H",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706364482660,
     "user_tz": -60,
     "elapsed": 284,
     "user": {
      "displayName": "Lennard Zündorf",
      "userId": "01714560816823084743"
     }
    }
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# running special model view with GODEL\n",
    "mview_enc_dec(\n",
    "    gd_format_prompt(\n",
    "        \"Does money buy happiness?\",\n",
    "        \"Given a dialog context, you need to respond empathically.\",\n",
    "    ),\n",
    "    gd_model,\n",
    "    gd_tokenizer,\n",
    ")"
   ],
   "metadata": {
    "id": "cBilWpaPzlwk",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1qyIungs2h2-y0XJW4LLASOFhRlygcHo9"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706364499014,
     "user_tz": -60,
     "elapsed": 12952,
     "user": {
      "displayName": "Lennard Zündorf",
      "userId": "01714560816823084743"
     }
    },
    "outputId": "bca613bc-6d7c-4475-d7c8-0af09ab7d153"
   },
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Output hidden; open in https://colab.research.google.com to view."
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing BERTViz with MistralAI"
   ],
   "metadata": {
    "id": "l-uEUd9RfAAO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# formatting function to format input for the model\n",
    "# CREDIT: Inspired by offical documentation and example on Huggingface\n",
    "## see https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
    "def mistral_format_prompt(message: str, system_prompt: str):\n",
    "    prompt = (\n",
    "        f\"<s>[INST] {system_prompt} [/INST] Hello, how can I assist you\"\n",
    "        f\" today?</s>[INST] {message} [/INST]\"\n",
    "    )\n",
    "    return prompt"
   ],
   "metadata": {
    "id": "FBX_tTIY9zY4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# running model with with mistral\n",
    "mistral_test_prompt = mistral_format_prompt(\n",
    "    \"Does money buy happiness?\",\n",
    "    \"Given a dialog context, you need to respond empathically.\",\n",
    ")\n",
    "mview(mistral_test_prompt, mistra_model, mistral_tokenizer)"
   ],
   "metadata": {
    "id": "UCf4mS-T9z6F",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1705925079724,
     "user_tz": -60,
     "elapsed": 11141,
     "user": {
      "displayName": "Lennard Zündorf",
      "userId": "01714560816823084743"
     }
    },
    "outputId": "33baddf5-cc74-49c6-b5bf-842a6e365d07"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "      \n",
       "        <div id=\"bertviz-5afa1245ab9f428581c1c7f950f5457e\" style=\"font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
       "            <span style=\"user-select:none\">\n",
       "                \n",
       "            </span>\n",
       "            <div id='vis'></div>\n",
       "        </div>\n",
       "    "
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# running head view with mistral\n",
    "mistral_test_prompt = mistral_format_prompt(\n",
    "    \"Does money buy happiness?\",\n",
    "    \"Given a dialog context, you need to respond empathically.\",\n",
    ")\n",
    "hview(mistral_test_prompt, mistra_model, mistral_tokenizer)"
   ],
   "metadata": {
    "id": "RPjA1oE-ySCD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing BERTViz with Llama2"
   ],
   "metadata": {
    "id": "So3OdMJu-_hN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# formatting function to format input for the model\n",
    "# CREDIT: Adapted from Philipp Schmid\n",
    "## see https://www.philschmid.de/llama-2#how-to-prompt-llama-2-chat\n",
    "def llama_format_prompt(message: str, system_prompt: str):\n",
    "    prompt = f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{message} [/INST]\"\n",
    "    return prompt"
   ],
   "metadata": {
    "id": "zcerCWmaAWMI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# running model view with llama 2\n",
    "llama_test_prompt = mistral_format_prompt(\n",
    "    \"Does money buy happiness?\",\n",
    "    \"Given a dialog context, you need to respond empathically.\",\n",
    ")\n",
    "mview(llama_test_prompt, llama_model, llama_tokenizer)"
   ],
   "metadata": {
    "id": "v-E56l8CyTfw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# running head view with llama 2\n",
    "mistral_test_prompt = mistral_format_prompt(\n",
    "    \"Does money buy happiness?\",\n",
    "    \"Given a dialog context, you need to respond empathically.\",\n",
    ")\n",
    "hview(llama_test_prompt, llama_model, llama_tokenizer)"
   ],
   "metadata": {
    "id": "gNwsU8y8yTwe"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "collapsed_sections": [
    "Bh8YtbD_aqs1",
    "J2Ih6vDGNkZr",
    "l-uEUd9RfAAO"
   ],
   "authorship_tag": "ABX9TyPVP0Xz6kXZOqi5Gn0Iq8B1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
